{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **Transform** is an object that \n",
    "- behaves like a function\n",
    "- has an optional setup method that will initialize some inner state \n",
    "- has an optional decode that will reverse the function (this reversal may not be perfect)\n",
    "\n",
    "These steps are needed for most data preprocessing tasks, so fastai provides a class that encapsulates them.\n",
    "\n",
    "In general, our data is always a tuple (input,target) (sometimes with more than one input or more than one target).\n",
    "A special behavior of Transforms is that they always get applied over tuples. \n",
    "\n",
    "When applying a transform on an item, we don't want to resize the tuple as a whole; instead, we want to resize the input (if applicable) and the target (if applicable) separately.\n",
    "\n",
    "It's the same for batch transforms that do data augmentation: when the input is an image and the target is a segmentation mask, the transform needs to be applied (the same way) to the input and the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transform:\n",
    "    def setups(self, items): \n",
    "    def encodes(self, x):\n",
    "    def decodes(self, x):\n",
    "        \n",
    "tfm = Transform()\n",
    "tfm.setup([...])\n",
    "x2 = tfm(x1)\n",
    "x1 = tfm.decode(x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compose several transforms together, fastai provides the **Pipeline** class.\n",
    "\n",
    "We define a Pipeline by passing it a list of Transforms; it will then compose the transforms inside it. When you call Pipeline on an object, it will automatically call the transforms inside, in order.\n",
    "\n",
    "The only part that doesn't work the same way as in Transform is the setup. To properly set up a Pipeline of Transforms on some data, you need to use a TfmdLists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = Pipeline([tfm1, tfm2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your data is usually a set of raw items (like filenames, or rows in a DataFrame) to which you want to apply a succession of transformations. We just saw that a succession of transformations is represented by a Pipeline in fastai. \n",
    "\n",
    "The class that groups together this Pipeline with your raw items is called **TfmdLists**.\n",
    "\n",
    "At initialization, the TfmdLists will automatically call the setup method of each Transform in order, providing them not with the raw items but the items transformed by all the previous Transforms in order. \n",
    "\n",
    "We can get the result of our Pipeline on any raw element just by indexing into the TfmdLists.\n",
    "\n",
    "The TfmdLists is named with an \"s\" because it can handle a training and a validation set with a splits argument. You just need to pass the indices of which elements are in the training set, and which are in the validation set. You can then access them through the train and valid attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut = int(len(items)*0.8)\n",
    "splits = [list(range(cut)), list(range(cut,len(items)))]\n",
    "\n",
    "tls = TfmdLists(items, [tfm1, tfm2], splits=splits)\n",
    "x2 = tls.train[0]\n",
    "x1 = tls.decode(x2)\n",
    "tls.show(x1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But then we end up with two separate objects for our inputs and targets, which is not what we want.\n",
    "\n",
    "**Datasets** will apply two (or more) pipelines in parallel to the same raw object and build a tuple with the result.\n",
    "\n",
    "Like TfmdLists, it will automatically do the setup.\n",
    "\n",
    "Like TfmdLists, we can pass along splits to split our data between training and validation sets.\n",
    "\n",
    "When we index into a Datasets, it will return us a tuple with the results of each pipeline.\n",
    "\n",
    "It can also decode any processed tuple or show it directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsets = Datasets(items, [x_tfms, y_tfms], splits=splits)\n",
    "x,y = dsets.valid[0]\n",
    "dsets.decode((x,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step is to convert our Datasets object to a **DataLoaders**, which can be done with the dataloaders method. \n",
    "\n",
    "dataloaders directly calls DataLoader on each subset of our Datasets. fastai's DataLoader expands the PyTorch class of the same name and is responsible for collating the items from our datasets into batches. \n",
    "\n",
    "It has a lot of points of customization, but the most important ones are:\n",
    "- after_item : Applied on each item after grabbing it inside the dataset.\n",
    "- before_batch : Applied on the list of items before they are collated. This is the ideal place to pad items to the same size.\n",
    "- after_batch : Applied on the batch as a whole after its construction.\n",
    "\n",
    "The dl_type argument tells dataloaders to use the SortedDL class of DataLoader, and not the usual one. SortedDL constructs batches by putting samples of roughly the same lengths into batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = dsets.dataloaders(dl_type=SortedDL, before_batch=pad_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Dataloaders.test_dl() method to create a batch from indivdual items :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.dls.test_dl([item], rm_type_tfms=rm_type_tfms, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type transforms :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformBlock():\n",
    "    \"A basic wrapper that links defaults transforms for the data block API\"\n",
    "    \n",
    "    def __init__(self, type_tfms=None, item_tfms=None, batch_tfms=None, dl_type=None, dls_kwargs={}):\n",
    "        self.type_tfms  =            L(type_tfms)\n",
    "        self.item_tfms  = ToTensor + L(item_tfms)\n",
    "        self.batch_tfms =            L(batch_tfms)\n",
    "        self.dl_type    =              dl_type\n",
    "        self.dls_kwargs =              dls_kwargs          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CategoryBlock(vocab=None, sort=True, add_na=False):\n",
    "    \"`TransformBlock` for single-label categorical targets\"\n",
    "    \n",
    "    type_tfms=Categorize(vocab=vocab, sort=sort, add_na=add_na)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MultiCategoryBlock(encoded=False, vocab=None, add_na=False):\n",
    "    \"`TransformBlock` for multi-label categorical targets\"\n",
    "    \n",
    "    if encoded:\n",
    "        type_tfms=EncodedMultiCategorize(vocab=vocab) \n",
    "    else:\n",
    "        type_tfms=[MultiCategorize(vocab=vocab, add_na=add_na), OneHotEncode]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RegressionBlock(n_out=None):\n",
    "    \"`TransformBlock` for float targets\"\n",
    "    \n",
    "    type_tfms=RegressionSetup(c=n_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataBlock():\n",
    "    \"Generic container to quickly build `Datasets` and `DataLoaders`\"\n",
    "       \n",
    "    source =>  Datasets.items\n",
    "    get_items => Datasets.items\n",
    "    splitter =>  Datasets.splits    \n",
    "    \n",
    "    blocks = (TransformBlock,TransformBlock)*\n",
    "    n_inp =>  Datasets.n_inp\n",
    "    \n",
    "    getters =>  Datasets.tfms\n",
    "    type_tfms* =>  Datasets.tfms    \n",
    "    \n",
    "    default_item_tfms* => Dataloaders.after_item\n",
    "    item_tfms => Dataloaders.after_item\n",
    "    \n",
    "    default_batch_tfms* => Dataloaders.after_batch\n",
    "    batch_tfms => Dataloaders.after_batch\n",
    "    \n",
    "    dl_type = TfmdDL* =>  Datasets.dl_type    \n",
    "    -> dataloaders\n",
    "    dls_kwargs* => Dataloaders.kwargs\n",
    "    \n",
    "    def __init__(self, blocks=None, dl_type=None, getters=None, n_inp=None, item_tfms=None, batch_tfms=None):\n",
    "        # Properties initialized by blocks\n",
    "        blocks = L(b() if callable(b) else b for b in blocks)\n",
    "        self.type_tfms = blocks.attrgot('type_tfms', L())\n",
    "        \n",
    "        self.default_item_tfms  = _merge_tfms(*blocks.attrgot('item_tfms',  L()))\n",
    "        self.default_batch_tfms = _merge_tfms(*blocks.attrgot('batch_tfms', L()))\n",
    "        \n",
    "        for b in blocks: \n",
    "            if getattr(b, 'dl_type', None) is not None: self.dl_type = b.dl_type\n",
    "        if dl_type is not None: self.dl_type = dl_type\n",
    "        self.dataloaders = delegates(self.dl_type.__init__)(self.dataloaders)\n",
    "        self.dls_kwargs = merge(*blocks.attrgot('dls_kwargs', {}))\n",
    "        \n",
    "        # Pipeline\n",
    "        self.n_inp = ifnone(n_inp, max(1, len(blocks)-1))\n",
    "        self.getters = ifnone(getters, [noop]*len(self.type_tfms))\n",
    "        if self.get_x:\n",
    "            self.getters[:self.n_inp] = L(self.get_x)\n",
    "        if self.get_y:\n",
    "            self.getters[self.n_inp:] = L(self.get_y)\n",
    "        self.new(item_tfms, batch_tfms)\n",
    "        \n",
    "    def new(self, item_tfms=None, batch_tfms=None):\n",
    "        \"Create a new `DataBlock` with other `item_tfms` and `batch_tfms`\"\n",
    "        self.item_tfms  = _merge_tfms(self.default_item_tfms,  item_tfms)\n",
    "        self.batch_tfms = _merge_tfms(self.default_batch_tfms, batch_tfms)\n",
    "        \n",
    "    def datasets(self, source, verbose=False):\n",
    "        \"Create a `Datasets` object from `source`\"\n",
    "        self.source = source                     \n",
    "        items = (self.get_items or noop)(source)\n",
    "        splits = (self.splitter or RandomSplitter())(items)  \n",
    "        return Datasets(items, tfms=self._combine_type_tfms(), splits=splits, dl_type=self.dl_type, n_inp=self.n_inp, verbose=verbose)\n",
    "    \n",
    "    def dataloaders(self, source, path='.', verbose=False, **kwargs):\n",
    "        dsets = self.datasets(source)\n",
    "        kwargs = {**self.dls_kwargs, **kwargs, 'verbose': verbose}\n",
    "        return dsets.dataloaders(path=path, after_item=self.item_tfms, after_batch=self.batch_tfms, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(file, model, opt, with_opt=True, pickle_protocol=2)\n",
    "def load_model(file, model, opt, with_opt=None, device=None, strict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "file can be a Path object, a string or an opened file object. \n",
    "\n",
    "pickle_protocol is passed along to torch.save.\n",
    "\n",
    "If a device is passed, the model is loaded on it, otherwise it's loaded on the CPU.\n",
    "\n",
    "If strict is True, the file must exactly contain weights for every parameter key in model, if strict is False, only the keys that are in the saved model are loaded in model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss function & Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_args\n",
    "class BBoxLblLoss(Module):    \n",
    "    def __init__(self):\n",
    "        self.reduction = 'mean'\n",
    "        self.l1_loss = F.l1_loss\n",
    "        self.binary_cross_entropy = F.binary_cross_entropy_with_logits\n",
    "\n",
    "    def forward(self, activations, bbox_target, label_target):        \n",
    "        bbox_activations, label_activations = activations\n",
    "        encoded_label_target = one_hot_encode_class(label_target, 20)\n",
    "        bbox_loss = self.l1_loss(bbox_activations, bbox_target.squeeze(1), reduction=self.reduction)            \n",
    "        label_loss = self.binary_cross_entropy(label_activations, encoded_label_target, reduction=self.reduction)        \n",
    "        if self.reduction=='none':\n",
    "            bbox_loss = bbox_loss.mean(1)\n",
    "            label_loss = label_loss.mean(1)\n",
    "        return bbox_loss + 20*label_loss # scale the two numbers in the same range\n",
    "    \n",
    "    def activation(self, activations):               \n",
    "        bbox_activations, label_activations = activations\n",
    "        return bbox_activations, torch.sigmoid(label_activations)\n",
    "    \n",
    "    def decodes(self, activations):\n",
    "        bbox_activations, label_activations = activations\n",
    "        return bbox_activations, (torch.argmax(label_activations,1)+1).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learner.create_opt()\n",
    "\n",
    "Creates an optimizer with default hyper-parameters\n",
    "\n",
    "This method is called internally to create the optimizer, the hyper-parameters are then adjusted by what you pass to Learner.fit or your particular schedulers (see callback.schedule)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learner - training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loop is defined in Learner a bit below and consists in a minimal set of instructions: looping through the data we:\n",
    "- compute the output of the model from the input\n",
    "- calculate a loss between this output and the desired target\n",
    "- compute the gradients of this loss with respect to all the model parameters\n",
    "- update the parameters accordingly\n",
    "- zero all the gradients\n",
    "\n",
    "Any tweak of this training loop is defined in a Callback to avoid over-complicating the code of the training loop, and to make it easy to mix and match different techniques (since they'll be defined in different callbacks). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learner groups together a model, dataloaders and a loss_func to handle training.\n",
    "\n",
    "opt_func will be used to create an optimizer when Learner.fit is called, with lr as a default learning rate. \n",
    "\n",
    "splitter is a function that takes self.model and returns a list of parameter groups (or just one parameter group if there are no different parameter groups). The default is trainable_params, which returns all trainable parameters of the model.\n",
    "\n",
    "cbs is one or a list of Callbacks to pass to the Learner. Callbacks are used for every tweak of the training loop. Each Callback is registered as an attribute of Learner (with camel case). At creation, all the callbacks in defaults.callbacks (TrainEvalCallback, Recorder and ProgressCallback) are associated to the Learner.\n",
    "\n",
    "metrics is an optional list of metrics, that can be either functions or Metrics (see below).\n",
    "\n",
    "path and model_dir are used to save and/or load models. Often path will be inferred from dls, but you can override it or pass a Path object to model_dir. Make sure you can write in path/model_dir!\n",
    "\n",
    "wd is the default weight decay used when training the model; moms, the default momentums used in Learner.fit_one_cycle. wd_bn_bias controls if weight decay is applied to BatchNorm layers and bias.\n",
    "\n",
    "Lastly, train_bn controls if BatchNorm layers are trained even when they are supposed to be frozen according to the splitter. Our empirical experiments have shown that it's the best behavior for those layers in transfer learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use regular PyTorch functionality for most of the arguments of the Learner, although the experience will be smoother with pure fastai objects and you will be able to use the full functionality of the library. The expectation is that the training loop will work smoothly even if you did not use fastai end to end. What you might lose are interpretation objects or showing functionality. The list below explains how to use plain PyTorch objects for all the arguments and what you might lose.\n",
    "\n",
    "The most important is opt_func. If you are not using a fastai optimizer, you will need to write a function that wraps your PyTorch optimizer in an OptimWrapper. See the optimizer module for more details. This is to ensure the library's schedulers/freeze API work with your code.\n",
    "\n",
    "dls is a DataLoaders object, that you can create from standard PyTorch dataloaders. By doing so, you will lose all showing functionality like show_batch/show_results. You can check the data block API or the mid-level data API tutorial to learn how to use fastai to gather your data!\n",
    "\n",
    "model is a standard PyTorch model. You can use anyone you like, just make sure it accepts the number of inputs you have in your DataLoaders and returns as many outputs as you have targets.\n",
    "\n",
    "loss_func can be any loss function you like. It needs to be one of fastai's if you want to use Learn.predict or Learn.get_preds, or you will have to implement special methods (see more details after the BaseLoss documentation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner():\n",
    "    \"Group together a `model`, some `dls` and a `loss_func` to handle training\"\n",
    "        \n",
    "    path = if path is None: dls.path or Path('.')\n",
    "    model_dir = 'models'\n",
    "    \n",
    "    dls\n",
    "    model -> model.to(dls.device) / if hasattr(model,'reset') model.reset()\n",
    "    splitter = trainable_params   \n",
    "    loss_func = if None: dls.train_ds.loss_func \n",
    "                else:  \"Could not infer loss function from the data\"\n",
    "    \n",
    "    opt_func = Adam\n",
    "    lr = defaults.lr\n",
    "    wd = None\n",
    "    wd_bn_bias = False\n",
    "    train_bn = True\n",
    "    moms = (0.95,0.85,0.95)\n",
    "    opt = None\n",
    "    \n",
    "    metrics -> _metrics / _metrics = L(v).map(mk_metric)\n",
    "    \n",
    "    training = False\n",
    "    cbs = L(defaults.callbacks)+L(cbs)\n",
    "       \n",
    "    loss = tensor(0.)\n",
    "    n_epoch = 1\n",
    "    epoch = 0\n",
    "    dl = dls.train / sdls.valid\n",
    "    n_iter = len(dl)\n",
    "    iter = i\n",
    "    xb,yb = _split(b): self.dls.n_inp or len(b)-1\n",
    "    pred = model(*xb)\n",
    "    loss = loss_func(pred, *yb)\n",
    "    \n",
    "    def __init__(self, dls, model, loss_func=None, opt_func=Adam, lr=defaults.lr, splitter=trainable_params, cbs=None,\n",
    "                 metrics=None, path=None, model_dir='models', wd=None, wd_bn_bias=False, train_bn=True,\n",
    "                 moms=(0.95,0.85,0.95)):\n",
    " \n",
    "    Learner.x,Learner.y = add_props(lambda i,x: detuplify((x.xb,x.yb)[i]))\n",
    "\n",
    "    @delegates(save_model)\n",
    "    def save(self, file, **kwargs):\n",
    "        \"Save model and optimizer state (if `with_opt`) to `self.path/self.model_dir/file`\" \n",
    "    @delegates(load_model)\n",
    "    def load(self, file, with_opt=None, device=None, **kwargs):\n",
    "        \"Load model and optimizer state (if `with_opt`) from `self.path/self.model_dir/file` using `device`\"\n",
    "    \n",
    "     \n",
    "        \n",
    "    def add_cbs(self, cbs):\n",
    "        \"Add `cbs` to the list of `Callback` and register `self` as their learner\"\n",
    "    def add_cb(self, cb):\n",
    "        \"Add `cb` to the list of `Callback` and register `self` as their learner\"\n",
    "    def remove_cbs(self, cbs):\n",
    "        \"Remove `cbs` from the list of `Callback` and deregister `self` as their learner\"\n",
    "    def remove_cb(self, cb):\n",
    "        \"Add `cb` from the list of `Callback` and deregister `self` as their learner\"\n",
    "    @contextmanager\n",
    "    def added_cbs(self, cbs):\n",
    "        \"Context manager that temporarily adds `cbs`\"      \n",
    "    @contextmanager\n",
    "    def removed_cbs(self, cbs):\n",
    "        \"Context manage that temporarily removes `cbs`\"\n",
    "    def ordered_cbs(self, event):\n",
    "        \"Return the list of `Callback`, in order, for an `event` in the training loop\"     \n",
    "    def __call__(self, event_name): \n",
    "        \"Call `event_name` for all `Callback`s in `self.cbs`\"\n",
    "\n",
    "    def show_training_loop(self):\n",
    "        \"Show each step in the training loop\n",
    "        \"At each step, callbacks are shown in order, which can help debugging.\n",
    "        else: print(f'{\" \"*indent} - {s:15}:', self.ordered_cbs(s))  \n",
    "\n",
    "\n",
    "\n",
    "    def fit(self, n_epoch, lr=None, wd=None, cbs=None, reset_opt=False):\n",
    "        with self.added_cbs(cbs):\n",
    "            # Create optimizer\n",
    "            self.create_opt()          # if reset_opt or not self.opt: self.create_opt()\n",
    "            self.opt.set_hypers(wd=wd) # if wd is None: wd = self.wd \n",
    "            self.opt.set_hypers(lr=lr) # self.lr if lr is None else lr\n",
    "\n",
    "            try:\n",
    "                #_do_begin_fit\n",
    "                self.n_epoch,self.loss = n_epoch,tensor(0.);         self('begin_fit')\n",
    "                for epoch in range(n_epoch):\n",
    "                    try:\n",
    "                        self.epoch=epoch;          self('begin_epoch')\n",
    "                        #_do_epoch_train\n",
    "                        try:\n",
    "                            self.dl = self.dls.train;                        self('begin_train')\n",
    "                            self.all_batches()\n",
    "                        except CancelTrainException:                         self('after_cancel_train')\n",
    "                        finally:                                             self('after_train')\n",
    "                        #_do_epoch_validate\n",
    "                        try:\n",
    "                            self.dl = self.dls.valid                         self('begin_validate')\n",
    "                            with torch.no_grad(): self.all_batches()\n",
    "                        except CancelValidException:                         self('after_cancel_validate')\n",
    "                        finally:                                             self('after_validate')\n",
    "                    except CancelEpochException:   self('after_cancel_epoch')\n",
    "                    finally:                       self('after_epoch')\n",
    "\n",
    "            except CancelFitException:             self('after_cancel_fit')\n",
    "            finally:\n",
    "                self('after_fit')\n",
    "                self.dl,self.xb,self.yb,self.pred,self.loss = None,(None,),(None,),None,None                                          \n",
    "    \n",
    "    def create_opt(self):\n",
    "        \"Create an optimizer with default hyper-parameters\"\n",
    "        self.opt = self.opt_func(self.splitter(self.model), lr=self.lr)\n",
    "        if not self.wd_bn_bias:\n",
    "            for p in bn_bias_params(self.model, True).map(self.opt.state): p['do_wd'] = False\n",
    "        if self.train_bn:\n",
    "            for p in bn_bias_params(self.model, False).map(self.opt.state): p['force_train'] = True\n",
    "    \n",
    "    def all_batches(self):\n",
    "        self.n_iter = len(self.dl)\n",
    "        for o in enumerate(self.dl): self.one_batch(*o)\n",
    "\n",
    "    def one_batch(self, i, b):\n",
    "        self.iter = i\n",
    "        try:\n",
    "            self._split(b);                                  self('begin_batch')\n",
    "            self.pred = self.model(*self.xb);                self('after_pred')\n",
    "            if len(self.yb) == 0: return # <-- inference\n",
    "            self.loss = self.loss_func(self.pred, *self.yb); self('after_loss')\n",
    "            if not self.training: return # <-- validation\n",
    "            self.loss.backward();                            self('after_backward')\n",
    "            self.opt.step();                                 self('after_step')\n",
    "            self.opt.zero_grad()\n",
    "        except CancelBatchException:                         self('after_cancel_batch')\n",
    "        finally:                                             self('after_batch')\n",
    "                    \n",
    "    def validate(self, ds_idx=1, dl=None, cbs=None):\n",
    "        if dl is None: dl = self.dls[ds_idx]\n",
    "        with self.added_cbs(cbs), self.no_logging(), self.no_mbar():\n",
    "            self('begin_fit')\n",
    "            self('begin_epoch')\n",
    "            if dl is None: dl = self.dls[ds_idx]\n",
    "            try:\n",
    "                self.dl = dl;                                    self('begin_validate')\n",
    "                with torch.no_grad(): self.all_batches()\n",
    "            except CancelValidException:                         self('after_cancel_validate')\n",
    "            finally:                                             self('after_validate')\n",
    "            self('after_epoch')\n",
    "            self('after_fit')\n",
    "        return getattr(self, 'final_record', None)          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Callback(GetAttr):\n",
    "    \"Basic class handling tweaks of the training loop by changing a `Learner` in various events\"\n",
    "    learn = None\n",
    "    run = True\n",
    "    run_train = True\n",
    "    run_valid = True\n",
    "\n",
    "    def __call__(self, event_name):\n",
    "        \"Call `self.{event_name}` if it's defined\"\n",
    "        if self.run and _run: getattr(self, event_name, noop)()\n",
    "        if event_name=='after_fit': self.run=True #Reset self.run to True at each end of fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Callback.name\n",
    "\n",
    "Name of the Callback, camel-cased and with 'Callback' removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Callbacks become attributes of Learner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(...)\n",
    "assert isinstance(learn.cbs[0], TrainEvalCallback)\n",
    "assert hasattr(tst_learn, ('train_eval'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a shortcut to avoid having to write self.learn.bla for any bla attribute we seek: you can just write self.bla.\n",
    "\n",
    "Note that it only works to get the value of the attribute, if you want to change it, you have to manually access it with self.learn.bla. self.a += 1 creates an a attribute of 2 in the callback instead of setting the a of the learner to 2. It also issues a warning that something is probably wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A callback can implement actions on the following events:\n",
    "- begin_fit: called before doing anything, ideal for initial setup.\n",
    "- begin_epoch: called at the beginning of each epoch, useful for any behavior you need to reset at each epoch.\n",
    "- begin_train: called at the beginning of the training part of an epoch.\n",
    "- begin_batch: called at the beginning of each batch, just after drawing said batch. It can be used to do any setup necessary for the batch (like hyper-parameter scheduling) or to change the input/target before it goes in the model (change of the input with techniques like mixup for instance).\n",
    "- after_pred: called after computing the output of the model on the batch. It can be used to change that output before it's fed to the loss.\n",
    "- after_loss: called after the loss has been computed, but before the backward pass. It can be used to add any penalty to the loss (AR or TAR in RNN training for instance).\n",
    "- after_backward: called after the backward pass, but before the update of the parameters. It can be used to do any change to the gradients before said update (gradient clipping for instance).\n",
    "- after_step: called after the step and before the gradients are zeroed.\n",
    "- after_batch: called at the end of a batch, for any clean-up before the next one.\n",
    "- after_train: called at the end of the training phase of an epoch.\n",
    "- begin_validate: called at the beginning of the validation phase of an epoch, useful for any setup needed specifically for validation.\n",
    "- after_validate: called at the end of the validation part of an epoch.\n",
    "- after_epoch: called at the end of an epoch, for any clean-up before the next one.\n",
    "- after_fit: called at the end of training, for final clean-up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yhe following attributes of Learner are available and updated during the training loop:\n",
    "\n",
    "- model: the model used for training/validation\n",
    "- dls: the underlying DataLoaders\n",
    "- loss_func: the loss function used\n",
    "- opt: the optimizer used to udpate the model parameters\n",
    "- opt_func: the function used to create the optimizer\n",
    "- cbs: the list containing all Callbacks\n",
    "- dl: current DataLoader used for iteration\n",
    "- x/xb: last input drawn from self.dl (potentially modified by callbacks). xb is always a tuple (potentially with one element) and x is detuplified. You can only assign to xb.\n",
    "- y/yb: last target drawn from self.dl (potentially modified by callbacks). yb is always a tuple (potentially with one element) and y is detuplified. You can only assign to yb.\n",
    "- pred: last predictions from self.model (potentially modified by callbacks)\n",
    "- loss: last computed loss (potentially modified by callbacks)\n",
    "- n_epoch: the number of epochs in this training\n",
    "- n_iter: the number of iterations in the current self.dl\n",
    "- epoch: the current epoch index (from 0 to n_epoch-1)\n",
    "- iter: the current iteration index in self.dl (from 0 to n_iter-1)\n",
    "T\n",
    "he following attributes are added by TrainEvalCallback and should be available unless you went out of your way to remove that callback:\n",
    "- train_iter: the number of training iterations done since the beginning of this training\n",
    "- pct_train: from 0. to 1., the percentage of training iterations completed\n",
    "- training: flag to indicate if we're in training mode or not\n",
    "\n",
    "The following attribute is added by Recorder and should be available unless you went out of your way to remove that callback:\n",
    "- smooth_loss: an exponentially-averaged version of the training loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainEvalCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, metrics are computed on the validation set only, although that can be changed by adjusting train_metrics and valid_metrics. \n",
    "\n",
    "beta is the weight used to compute the exponentially weighted average of the losses (which gives the smooth_loss attribute to Learner).\n",
    "\n",
    "The logger attribute of a Learner determines what happens to those metrics. By default, it just prints them.\n",
    "\n",
    "Recorder.plot_loss(skip_start=5, with_valid=True)\n",
    "\n",
    "Plots the losses from skip_start and onward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Recorder(Callback):\n",
    "    \"Callback that registers statistics (lr, loss and metrics) during training\"\n",
    "    \n",
    "    remove_on_fetch = True\n",
    "    run_after = TrainEvalCallback\n",
    "    \n",
    "    add_time = True\n",
    "    train_metrics = False\n",
    "    valid_metrics = True\n",
    "    metric_names\n",
    "    \n",
    "    self.loss = AvgLoss()\n",
    "    self.smooth_loss = AvgSmoothLoss(beta=beta)\n",
    "    \n",
    "    start_epoch\n",
    "    log\n",
    "    lrs\n",
    "    iters\n",
    "    losses\n",
    "    values\n",
    "    final_record \n",
    "    \n",
    "    def __init__(self, add_time=True, train_metrics=False, valid_metrics=True, beta=0.98):\n",
    "        store_attr(self, 'add_time,train_metrics,valid_metrics')\n",
    "        self.loss,self.smooth_loss = AvgLoss(),AvgSmoothLoss(beta=beta)\n",
    "\n",
    "    def begin_fit(self):\n",
    "        \"Prepare state for training\"\n",
    "        self.lrs,self.iters,self.losses,self.values = [],[],[],[]\n",
    "        names = self.metrics.attrgot('name')\n",
    "        if self.train_metrics and self.valid_metrics:\n",
    "            names = L('loss') + names\n",
    "            names = names.map('train_{}') + names.map('valid_{}')\n",
    "        elif self.valid_metrics: names = L('train_loss', 'valid_loss') + names\n",
    "        else: names = L('train_loss') + names\n",
    "        if self.add_time: names.append('time')\n",
    "        self.metric_names = 'epoch'+names\n",
    "        self.smooth_loss.reset()\n",
    "\n",
    "    def after_batch(self):\n",
    "        \"Update all metrics and records lr and smooth loss in training\"\n",
    "        if len(self.yb) == 0: return\n",
    "        mets = self._train_mets if self.training else self._valid_mets\n",
    "        for met in mets: met.accumulate(self.learn)\n",
    "        if not self.training: return\n",
    "        self.lrs.append(self.opt.hypers[-1]['lr'])\n",
    "        self.losses.append(self.smooth_loss.value)\n",
    "        self.learn.smooth_loss = self.smooth_loss.value\n",
    "\n",
    "    def begin_epoch(self):\n",
    "        \"Set timer if `self.add_time=True`\"\n",
    "        self.cancel_train,self.cancel_valid = False,False\n",
    "        if self.add_time: self.start_epoch = time.time()\n",
    "        self.log = L(getattr(self, 'epoch', 0))\n",
    "\n",
    "    def begin_train   (self): self._train_mets[1:].map(Self.reset())\n",
    "    def begin_validate(self): self._valid_mets.map(Self.reset())\n",
    "    def after_train   (self): self.log += self._train_mets.map(_maybe_item)\n",
    "    def after_validate(self): self.log += self._valid_mets.map(_maybe_item)\n",
    "    def after_cancel_train(self):    self.cancel_train = True\n",
    "    def after_cancel_validate(self): self.cancel_valid = True\n",
    "\n",
    "    def after_epoch(self):\n",
    "        \"Store and log the loss/metric values\"\n",
    "        self.learn.final_record = self.log[1:].copy()\n",
    "        self.values.append(self.learn.final_record)\n",
    "        if self.add_time: self.log.append(format_time(time.time() - self.start_epoch))\n",
    "        self.logger(self.log)\n",
    "        self.iters.append(self.smooth_loss.count)\n",
    "\n",
    "    def plot_loss(self, skip_start=5, with_valid=True):\n",
    "        plt.plot(list(range(skip_start, len(self.losses))), self.losses[skip_start:], label='train')\n",
    "        if with_valid:\n",
    "            idx = (np.array(self.iters)<skip_start).sum()\n",
    "            plt.plot(self.iters[idx:], L(self.values[idx:]).itemgot(1), label='valid')\n",
    "            plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ProgressCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics can be simple averages (like accuracy) but sometimes their computation is a little bit more complex and can't be averaged over batches (like precision or recall), which is why we need a special class for them. For simple functions that can be computed as averages over batches, we can use the class AvgMetric, otherwise you'll need to implement the following methods.\n",
    "\n",
    "Note: If your Metric has state depending on tensors, don't forget to store it on the CPU to avoid any potential memory leaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metric():\n",
    "    \"Blueprint for defining a metric\"\n",
    "    \n",
    "    def reset(self): pass\n",
    "    \"Reset inner state to prepare for new computation\"\n",
    "    \n",
    "    def accumulate(self, learn): pass\n",
    "    \"Use `learn` to update the state with new results\"\n",
    "    \n",
    "    @property\n",
    "    def value(self): raise NotImplementedError\n",
    "    \"The value of the metric\"\n",
    "        \n",
    "    @property\n",
    "    def name(self): return class2attr(self, 'Metric')\n",
    "    \"Name of the `Metric`, camel-cased and with Metric removed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AvgMetric(Metric):\n",
    "    \"Average the values of `func` taking into account potential different batch sizes\"\n",
    "    def __init__(self, func):  self.func = func\n",
    "    def reset(self):           self.total,self.count = 0.,0\n",
    "    def accumulate(self, learn):\n",
    "        bs = find_bs(learn.yb)\n",
    "        self.total += to_detach(self.func(learn.pred, *learn.yb))*bs\n",
    "        self.count += bs\n",
    "    @property\n",
    "    def value(self): return self.total/self.count if self.count != 0 else None\n",
    "    @property\n",
    "    def name(self):  return self.func.func.__name__ if hasattr(self.func, 'func') else  self.func.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueMetric(Metric):\n",
    "    \"Use to include a pre-calculated metric value (for instance calculated in a `Callback`) and returned by `func`\"\n",
    "    def __init__(self, func, metric_name=None): store_attr(self, 'func, metric_name')\n",
    "        \n",
    "    @property\n",
    "    def value(self): return self.func()\n",
    "\n",
    "    @property\n",
    "    def name(self): return self.metric_name if self.metric_name else self.func.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AvgLoss(Metric):\n",
    "    \"Average the losses taking into account potential different batch sizes\"\n",
    "    def reset(self):           self.total,self.count = 0.,0\n",
    "    def accumulate(self, learn):\n",
    "        bs = find_bs(learn.yb)\n",
    "        self.total += to_detach(learn.loss.mean())*bs\n",
    "        self.count += bs\n",
    "    @property\n",
    "    def value(self): return self.total/self.count if self.count != 0 else None\n",
    "    @property\n",
    "    def name(self):  return \"loss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AvgSmoothLoss(Metric):\n",
    "    \"Smooth average of the losses (exponentially weighted with `beta`)\"\n",
    "    def __init__(self, beta=0.98): self.beta = beta\n",
    "    def reset(self):               self.count,self.val = 0,tensor(0.)\n",
    "    def accumulate(self, learn):\n",
    "        self.count += 1\n",
    "        self.val = torch.lerp(to_detach(learn.loss.mean(), gather=False), self.val, self.beta)\n",
    "    @property\n",
    "    def value(self): return self.val/(1-self.beta**self.count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "def freeze_to(self:Learner, n):\n",
    "    \"Freeze parameter groups up to `n`\"\n",
    "    if self.opt is None: self.create_opt()\n",
    "    self.opt.freeze_to(n)\n",
    "    self.opt.clear_state()\n",
    "\n",
    "@patch\n",
    "def freeze(self:Learner): \n",
    "    \"Freeze up to last parameter group\"\n",
    "    self.freeze_to(-1)\n",
    "\n",
    "@patch\n",
    "def unfreeze(self:Learner): \n",
    "    \"Unfreeze the entire model\"\n",
    "    self.freeze_to(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learner - inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Learner.validate(ds_idx=1, dl=None, cbs=None)\n",
    "\n",
    "Validate on dl with potential new cbs.\n",
    "\n",
    "> Learner.get_preds(ds_idx=1, dl=None, with_input=False, with_decoded=False, with_loss=False, act=None, inner=False, reorder=True, save_preds=None, save_targs=None, concat_dim=0)\n",
    "\n",
    "Get the predictions and targets on the ds_idx-th dataset or dl, optionally with_input and with_loss\n",
    "\n",
    "with_decoded will also return the decoded predictions using the decodes function of the loss function (if it exists). For instance, fastai's CrossEntropyFlat takes the argmax or predictions in its decodes.\n",
    "\n",
    "Depending on the loss_func attribute of Learner, an activation function will be picked automatically so that the predictions make sense. For instance if the loss is a case of cross-entropy, a softmax will be applied, or if the loss is binary cross entropy with logits, a sigmoid will be applied. If you want to make sure a certain activation function is applied, you can pass it with act.\n",
    "\n",
    "save_preds and save_targs should be used when your predictions are too big to fit all in memory. Give a Path object that points to a folder where the predictions and targets will be saved.\n",
    "\n",
    "concat_dim is the batch dimension, where all the tensors will be concatenated.\n",
    "\n",
    "inner is an internal attribute that tells get_preds it's called internally, inside another training loop, to avoid recursion errors.\n",
    "\n",
    "Note: If you want to use the option with_loss=True on a custom loss function, make sure you have implemented a reduction attribute that supports 'none'\n",
    "\n",
    "> Learner.predict(item, rm_type_tfms=None, with_input=False)\n",
    "\n",
    "Return the prediction on item, fully decoded, loss function decoded and probabilities\n",
    "\n",
    "It returns a tuple of three elements with, in reverse order,\n",
    "\n",
    "- the prediction from the model, potentially passed through the activation of the loss function (if it has one)\n",
    "- the decoded prediction, using the poential decodes method from it\n",
    "- the fully decoded prediction, using the transforms used to buil the Datasets/DataLoaders\n",
    "\n",
    "rm_type_tfms is a deprecated argument that should not be used and will be removed in a future version. \n",
    "\n",
    "with_input will add the decoded inputs to the result.\n",
    "\n",
    "> Learner.show_results(ds_idx=1, dl=None, max_n=9, shuffle=True, **kwargs)\n",
    "\n",
    "Show some predictions on ds_idx-th dataset or dl\n",
    "\n",
    "Will show max_n samples (unless the batch size of ds_idx or dl is less than max_n, in which case it will show as many samples) and shuffle the data unless you pass false to that flag. kwargs are application-dependant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner(): \n",
    "                \n",
    "    def get_preds(self, ds_idx=1, dl=None, with_input=False, with_decoded=False, with_loss=False, act=None, \n",
    "                  inner=False, reorder=True, with_input=False, with_loss=False, save_preds=None, save_targs=None, \n",
    "                  concat_dim=0):\n",
    "        # Prepare dataloader\n",
    "        if dl is None: dl = self.dls[ds_idx].new(shuffled=False, drop_last=False)\n",
    "        if reorder and hasattr(dl, 'get_idxs'):\n",
    "            idxs = dl.get_idxs()\n",
    "            dl = dl.new(get_idxs = _ConstantFunc(idxs))\n",
    "           \n",
    "        # Prepare context\n",
    "        cb = GatherPredsCallback(with_input=with_input, with_loss=with_loss, **kwargs)\n",
    "        ctx_mgrs = [self.no_logging(), self.added_cbs(cb), self.no_mbar()]\n",
    "        if with_loss: ctx_mgrs.append(self.loss_not_reduced())\n",
    "        with ExitStack() as stack:\n",
    "            for mgr in ctx_mgrs: stack.enter_context(mgr)\n",
    "                \n",
    "            # Compute output of model\n",
    "            self(event.begin_epoch if inner else [event.begin_fit, event.begin_epoch])\n",
    "            try:\n",
    "                self.dl = dl;                                    self('begin_validate')\n",
    "                with torch.no_grad(): self.all_batches()\n",
    "            except CancelValidException:                         self('after_cancel_validate')\n",
    "            finally:                                             self('after_validate')\n",
    "            self(event.after_epoch if inner else _[event.after_epoch, event.after_fit])\n",
    "            \n",
    "            # Apply loss_func.activation then (optionally) loss_func.decodes on output\n",
    "            if act is None: act = getattr(self.loss_func, 'activation', noop)\n",
    "            res = cb.all_tensors()\n",
    "            pred_i = 1 if with_input else 0\n",
    "            if res[pred_i] is not None:\n",
    "                res[pred_i] = act(res[pred_i])\n",
    "                if with_decoded: res.insert(pred_i+2, getattr(self.loss_func, 'decodes', noop)(res[pred_i]))\n",
    "                    \n",
    "            # Reorder results\n",
    "            if reorder and hasattr(dl, 'get_idxs'): res = nested_reorder(res, tensor(idxs).argsort())\n",
    "            return tuple(res)\n",
    "        \n",
    "        # Cleanup\n",
    "        self.dl,self.xb,self.yb,self.pred,self.loss = None,(None,),(None,),None,None\n",
    "\n",
    "        \n",
    "    def predict(self, item, rm_type_tfms=None, with_input=False):\n",
    "        # Create dataloader from a single item\n",
    "        dl = self.dls.test_dl([item], rm_type_tfms=rm_type_tfms, num_workers=0)\n",
    "        \n",
    "        # Get decoded predictions\n",
    "        inp,preds,_,dec_preds = self.get_preds(dl=dl, with_input=True, with_decoded=True)\n",
    "        \n",
    "        # Dataloaders.decode_batch on (inputs + decoded preds) [tuplify / detuplify]\n",
    "        i = getattr(self.dls, 'n_inp', -1)\n",
    "        inp = (inp,) if i==1 else tuplify(inp)\n",
    "        dec = self.dls.decode_batch(inp + tuplify(dec_preds))[0]\n",
    "        dec_inp,dec_targ = map(detuplify, [dec[:i],dec[i:]])\n",
    "        res = dec_targ,dec_preds[0],preds[0]\n",
    "        \n",
    "        # Return\n",
    "        if with_input: res = (dec_inp,) + res\n",
    "        return res\n",
    "\n",
    "    \n",
    "    def show_results(self, ds_idx=1, dl=None, max_n=9, shuffle=True, **kwargs):\n",
    "        # Get one batch\n",
    "        if dl is None: dl = self.dls[ds_idx].new(shuffle=shuffle)\n",
    "        b = dl.one_batch()\n",
    "        \n",
    "        # Dataloaders.show_results on decoded preds\n",
    "        _,_,preds = self.get_preds(dl=[b], with_decoded=True)\n",
    "        self.dls.show_results(b, preds, max_n=max_n, **kwargs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tuplify(o, use_list=False, match=None)\n",
    "\n",
    "Make o a tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(tuplify(None),())\n",
    "test_eq(tuplify([1,2,3]),(1,2,3))\n",
    "test_eq(tuplify(1,match=[1,2,3]),(1,1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "detuplify(x)\n",
    "\n",
    "If x is a tuple with one thing, extract it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(detuplify(()),None)\n",
    "test_eq(detuplify([1]),1)\n",
    "test_eq(detuplify([1,2]), [1,2])\n",
    "test_eq(detuplify(np.array([[1,2]])), np.array([[1,2]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GatherPredsCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learner - export & import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "def export(self:Learner, fname='export.pkl', pickle_protocol=2):\n",
    "    \"Export the content of `self` without the items and the optimizer state for inference\"\n",
    "    if rank_distrib(): return # don't export if child proc\n",
    "    self._end_cleanup()\n",
    "    old_dbunch = self.dls\n",
    "    self.dls = self.dls.new_empty()\n",
    "    state = self.opt.state_dict() if self.opt is not None else None\n",
    "    self.opt = None\n",
    "    with warnings.catch_warnings():\n",
    "        #To avoid the warning that come from PyTorch about model not being checked\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        torch.save(self, self.path/fname, pickle_protocol=pickle_protocol)\n",
    "    self.create_opt()\n",
    "    if state is not None: self.opt.load_state_dict(state)\n",
    "    self.dls = old_dbunch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Learner is saved in self.path/fname, using pickle_protocol. Note that serialization in Python saves the names of functions, not the code itself. Therefore, any custom code you have for models, data transformation, loss function etc... should be put in a module that you will import in your training environment before exporting, and in your deployment environment before loading it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_learner(fname, cpu=True):\n",
    "    \"Load a `Learner` object in `fname`, optionally putting it on the `cpu`\"\n",
    "    distrib_barrier()\n",
    "    res = torch.load(fname, map_location='cpu' if cpu else None)\n",
    "    if hasattr(res, 'to_fp32'): res = res.to_fp32()\n",
    "    if cpu: res.dls.cpu()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warning: load_learner requires all your custom code be in the exact same place as when exporting your Learner (the main script, or the module you imported it from)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
