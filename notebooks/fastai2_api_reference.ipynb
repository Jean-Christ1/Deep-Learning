{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **Transform** is an object that \n",
    "- behaves like a function\n",
    "- has an optional setup method that will initialize some inner state \n",
    "- has an optional decode that will reverse the function (this reversal may not be perfect)\n",
    "\n",
    "These steps are needed for most data preprocessing tasks, so fastai provides a class that encapsulates them.\n",
    "\n",
    "In general, our data is always a tuple (input,target) (sometimes with more than one input or more than one target).\n",
    "A special behavior of Transforms is that they always get applied over tuples. \n",
    "\n",
    "When applying a transform on an item, we don't want to resize the tuple as a whole; instead, we want to resize the input (if applicable) and the target (if applicable) separately.\n",
    "\n",
    "It's the same for batch transforms that do data augmentation: when the input is an image and the target is a segmentation mask, the transform needs to be applied (the same way) to the input and the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transform:\n",
    "    def setups(self, items): \n",
    "    def encodes(self, x):\n",
    "    def decodes(self, x):\n",
    "        \n",
    "tfm = Transform()\n",
    "tfm.setup([...])\n",
    "x2 = tfm(x1)\n",
    "x1 = tfm.decode(x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compose several transforms together, fastai provides the **Pipeline** class.\n",
    "\n",
    "We define a Pipeline by passing it a list of Transforms; it will then compose the transforms inside it. When you call Pipeline on an object, it will automatically call the transforms inside, in order.\n",
    "\n",
    "The only part that doesn't work the same way as in Transform is the setup. To properly set up a Pipeline of Transforms on some data, you need to use a TfmdLists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = Pipeline([tfm1, tfm2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your data is usually a set of raw items (like filenames, or rows in a DataFrame) to which you want to apply a succession of transformations. We just saw that a succession of transformations is represented by a Pipeline in fastai. \n",
    "\n",
    "The class that groups together this Pipeline with your raw items is called **TfmdLists**.\n",
    "\n",
    "At initialization, the TfmdLists will automatically call the setup method of each Transform in order, providing them not with the raw items but the items transformed by all the previous Transforms in order. \n",
    "\n",
    "We can get the result of our Pipeline on any raw element just by indexing into the TfmdLists.\n",
    "\n",
    "The TfmdLists is named with an \"s\" because it can handle a training and a validation set with a splits argument. You just need to pass the indices of which elements are in the training set, and which are in the validation set. You can then access them through the train and valid attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut = int(len(items)*0.8)\n",
    "splits = [list(range(cut)), list(range(cut,len(items)))]\n",
    "\n",
    "tls = TfmdLists(items, [tfm1, tfm2], splits=splits)\n",
    "x2 = tls.train[0]\n",
    "x1 = tls.decode(x2)\n",
    "tls.show(x1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But then we end up with two separate objects for our inputs and targets, which is not what we want.\n",
    "\n",
    "**Datasets** will apply two (or more) pipelines in parallel to the same raw object and build a tuple with the result.\n",
    "\n",
    "Like TfmdLists, it will automatically do the setup.\n",
    "\n",
    "Like TfmdLists, we can pass along splits to split our data between training and validation sets.\n",
    "\n",
    "When we index into a Datasets, it will return us a tuple with the results of each pipeline.\n",
    "\n",
    "It can also decode any processed tuple or show it directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsets = Datasets(items, [x_tfms, y_tfms], splits=splits)\n",
    "x,y = dsets.valid[0]\n",
    "dsets.decode((x,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step is to convert our Datasets object to a **DataLoaders**, which can be done with the dataloaders method. \n",
    "\n",
    "dataloaders directly calls DataLoader on each subset of our Datasets. fastai's DataLoader expands the PyTorch class of the same name and is responsible for collating the items from our datasets into batches. \n",
    "\n",
    "It has a lot of points of customization, but the most important ones are:\n",
    "- after_item : Applied on each item after grabbing it inside the dataset.\n",
    "- before_batch : Applied on the list of items before they are collated. This is the ideal place to pad items to the same size.\n",
    "- after_batch : Applied on the batch as a whole after its construction.\n",
    "\n",
    "The dl_type argument tells dataloaders to use the SortedDL class of DataLoader, and not the usual one. SortedDL constructs batches by putting samples of roughly the same lengths into batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = dsets.dataloaders(dl_type=SortedDL, before_batch=pad_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformBlock():\n",
    "    \"A basic wrapper that links defaults transforms for the data block API\"\n",
    "    \n",
    "    def __init__(self, type_tfms=None, item_tfms=None, batch_tfms=None, dl_type=None, dls_kwargs={}):\n",
    "        self.type_tfms  =            L(type_tfms)\n",
    "        self.item_tfms  = ToTensor + L(item_tfms)\n",
    "        self.batch_tfms =            L(batch_tfms)\n",
    "        self.dl_type    =              dl_type\n",
    "        self.dls_kwargs =              dls_kwargs          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CategoryBlock(vocab=None, sort=True, add_na=False):\n",
    "    \"`TransformBlock` for single-label categorical targets\"\n",
    "    \n",
    "    type_tfms=Categorize(vocab=vocab, sort=sort, add_na=add_na)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MultiCategoryBlock(encoded=False, vocab=None, add_na=False):\n",
    "    \"`TransformBlock` for multi-label categorical targets\"\n",
    "    \n",
    "    if encoded:\n",
    "        type_tfms=EncodedMultiCategorize(vocab=vocab) \n",
    "    else:\n",
    "        type_tfms=[MultiCategorize(vocab=vocab, add_na=add_na), OneHotEncode]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RegressionBlock(n_out=None):\n",
    "    \"`TransformBlock` for float targets\"\n",
    "    \n",
    "    type_tfms=RegressionSetup(c=n_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataBlock():\n",
    "    \"Generic container to quickly build `Datasets` and `DataLoaders`\"\n",
    "       \n",
    "    source =>  Datasets.items\n",
    "    get_items => Datasets.items\n",
    "    splitter =>  Datasets.splits    \n",
    "    \n",
    "    blocks = (TransformBlock,TransformBlock)*\n",
    "    n_inp =>  Datasets.n_inp\n",
    "    \n",
    "    getters =>  Datasets.tfms\n",
    "    type_tfms* =>  Datasets.tfms    \n",
    "    \n",
    "    default_item_tfms* => Dataloaders.after_item\n",
    "    item_tfms => Dataloaders.after_item\n",
    "    \n",
    "    default_batch_tfms* => Dataloaders.after_batch\n",
    "    batch_tfms => Dataloaders.after_batch\n",
    "    \n",
    "    dl_type = TfmdDL* =>  Datasets.dl_type    \n",
    "    -> dataloaders\n",
    "    dls_kwargs* => Dataloaders.kwargs\n",
    "    \n",
    "    def __init__(self, blocks=None, dl_type=None, getters=None, n_inp=None, item_tfms=None, batch_tfms=None):\n",
    "        # Properties initialized by blocks\n",
    "        blocks = L(b() if callable(b) else b for b in blocks)\n",
    "        self.type_tfms = blocks.attrgot('type_tfms', L())\n",
    "        \n",
    "        self.default_item_tfms  = _merge_tfms(*blocks.attrgot('item_tfms',  L()))\n",
    "        self.default_batch_tfms = _merge_tfms(*blocks.attrgot('batch_tfms', L()))\n",
    "        \n",
    "        for b in blocks: \n",
    "            if getattr(b, 'dl_type', None) is not None: self.dl_type = b.dl_type\n",
    "        if dl_type is not None: self.dl_type = dl_type\n",
    "        self.dataloaders = delegates(self.dl_type.__init__)(self.dataloaders)\n",
    "        self.dls_kwargs = merge(*blocks.attrgot('dls_kwargs', {}))\n",
    "        \n",
    "        # Pipeline\n",
    "        self.n_inp = ifnone(n_inp, max(1, len(blocks)-1))\n",
    "        self.getters = ifnone(getters, [noop]*len(self.type_tfms))\n",
    "        if self.get_x:\n",
    "            self.getters[:self.n_inp] = L(self.get_x)\n",
    "        if self.get_y:\n",
    "            self.getters[self.n_inp:] = L(self.get_y)\n",
    "        self.new(item_tfms, batch_tfms)\n",
    "        \n",
    "    def new(self, item_tfms=None, batch_tfms=None):\n",
    "        \"Create a new `DataBlock` with other `item_tfms` and `batch_tfms`\"\n",
    "        self.item_tfms  = _merge_tfms(self.default_item_tfms,  item_tfms)\n",
    "        self.batch_tfms = _merge_tfms(self.default_batch_tfms, batch_tfms)\n",
    "        \n",
    "    def datasets(self, source, verbose=False):\n",
    "        \"Create a `Datasets` object from `source`\"\n",
    "        self.source = source                     \n",
    "        items = (self.get_items or noop)(source)\n",
    "        splits = (self.splitter or RandomSplitter())(items)  \n",
    "        return Datasets(items, tfms=self._combine_type_tfms(), splits=splits, dl_type=self.dl_type, n_inp=self.n_inp, verbose=verbose)\n",
    "    \n",
    "    def dataloaders(self, source, path='.', verbose=False, **kwargs):\n",
    "        dsets = self.datasets(source)\n",
    "        kwargs = {**self.dls_kwargs, **kwargs, 'verbose': verbose}\n",
    "        return dsets.dataloaders(path=path, after_item=self.item_tfms, after_batch=self.batch_tfms, **kwargs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
