{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copyright : fast.ai - Jeremy Howard & Sylvain Gugger - 2020 (GPLv3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cellules de code et plan du notebook adaptées du livre :\n",
    "\n",
    "Deep Learning for Coders with fastai & PyTorch de Jeremy Howard et Sylvain Gugger.\n",
    "\n",
    "The code in the original notebooks (and thus the code in this notebook) is covered by the GPL v3 license; see the LICENSE file for details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Deep Dive: RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons entrainer un **language model** : un modèle capable de deviner le mot suivant dans un texte à partir des mots qui le précèdent. \n",
    "\n",
    "Ce type de tâche est appelé **self-supervised learning** : nous n'avons pas besoin de créer explicitement des labels pour entrainer ce type de modèle, il suffit de le nourrir de beaucoup, beaucoup de textes. Pour cette tâche, nous avons un processus pour obtenir automatiquement les labels à prédire à partir des données brutes.\n",
    "\n",
    "Cette tâche n'est pas triviale : pour deviner correctement le mot suivant dans une phrase, le modèle devra développer une compréhension assez approfondie de la langue. \n",
    "\n",
    "Le **self-supervised learning** peut également être utilisé dans d'autres domaines, c'est un sujet de recherche très actif en ce moment, par exemple en vision : [Prototypical Contrastive Learning](https://blog.einstein.ai/prototypical-contrastive-learning-pushing-the-frontiers-of-unsupervised-learning/). \n",
    "\n",
    "Un modèle entrainé par **self-supervised learning** n'est en général pas utilisé directement : il sert de point de départ pour réaliser un fine-tuning en mode supervisé sur une beaucoup plus petite quantité de données."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons travailler sur le jeu de données [IMDb sentiment analysis](https://www.imdb.com/title/tt0107144/reviews?ref_=tt_ov_rt) :\n",
    "- 50.000 critiques de films sans labels positifs ou négatifs\n",
    "- 25 000 critiques avec des labels dans le jeu d'entrainement\n",
    "- 25 000 critiques avec des labels dans le jeu de validation\n",
    "- => 100 000 critiques de films au total\n",
    "\n",
    "Nous pouvons utiliser toutes ces critiques pour affiner un language model pré-entrainé sur les articles de Wikipédia : cela permettra d'obtenir un language model particulièrement efficace pour prédire le prochain mot d'une critique de film.\n",
    "\n",
    "Cette approche appelée [Universal Language Model Fine-tuning (ULMFit)](https://arxiv.org/abs/1801.06146) a été inventée lors du cours fastai 2017-2018. C'était le premier exemple de transfert d'apprentissage sur du texte. Des benchmarks sont régulièrement réalisés avec des méthodes plus modernes et beaucoup plus lourdes en termes de calcul : cette méthode reste un très bon compromis coût/performance pour la classification de textes.\n",
    "\n",
    "Cette publication a montré que l'étape supplémentaire de fine tuning du language model, avant de transférer l'apprentissage à une tâche de classification, permettait d'obtenir des prévisions nettement meilleures.\n",
    "\n",
    "En utilisant cette approche, nous avons trois étapes pour le transfert de l'apprentissage en NLP (Natural Language Processing) :\n",
    "1. Language model - self-supervised - entrainement initial : Wikitext 103\n",
    "2. Language mode - self-supervised - fine tuning : IMDb\n",
    "3. Classifier - supervisé - fine tuning : IMDb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons déjà vu comment les variables de type catégorie (énumération - liste de niveaux finie) peuvent être utilisées en entrée d'un réseau de neurones.\n",
    "\n",
    "L'approche que nous avons adoptée était la suivante :\n",
    "1. Faire une liste de toutes les valeurs possibles de cette catégorie (nous appellerons cette liste le vocabulaire).\n",
    "2. Remplacer chaque valeur par son index dans le vocabulaire.\n",
    "3. Créer une matrice d'embeddings contenant une représentation de chaque valeur (c'est-à-dire pour chaque élément du vocabulaire) sous forme d'une liste de nombres .\n",
    "4. Utiliser cette matrice d'embeddings comme première couche d'un réseau de neurones. \n",
    "\n",
    "Les mots de vocabulaire d'une langue donnée peuvent être traités comme les différents niveaux d'une variable de type catégorie : on va retenir une liste finie de mots et **apprendre une représentation numérique sous forme d'embedding pour chacun des mots du vocabulaire retenu**.\n",
    "\n",
    "Ce qui est nouveau, c'est l'idée d'une **séquence**. D'abord, nous concaténons tous les documents de notre jeu de données en une longue chaîne et nous la divisons en mots, ce qui nous donne une très longue liste de mots (ou \"tokens\"). La donnée d'entrée de notre modèle sera la séquence de mots commençant par le premier mot de notre très longue liste et se terminant par l'avant-dernier, et notre variable à prédire sera la séquence de mots commençant par le deuxième mot et se terminant par le dernier.\n",
    "\n",
    "Notre vocabulaire sera constitué d'un mélange de mots communs qui sont déjà dans le vocabulaire de notre modèle pré-entrainé sur Wikipedia et de nouveaux mots spécifiques à notre corpus (termes cinématographiques ou noms d'acteurs, par exemple). Notre matrice d'embeddings sera construite en conséquence : pour les mots qui sont dans le vocabulaire de notre modèle pré-entrainé, nous prendrons la ligne correspondante dans la matrice d'embedding du modèle pré-entrainé ; mais pour les nouveaux mots, nous n'aurons rien, donc nous initialiserons juste la ligne correspondante avec un vecteur aléatoire.\n",
    "\n",
    "Chacune des étapes nécessaires à la création d'un language model est associée à du jargon issu du monde du traitement du langage naturel, et des classes de fastai et PyTorch sont disponibles pour aider. Les étapes sont les suivantes :\n",
    "\n",
    "- Tokenization : Convertir le texte en une liste de mots (ou de caractères, ou de sous-chaînes de caractères, selon la granularité de votre modèle)\n",
    "- Numericalization : Faire une liste de tous les mots uniques qui apparaissent (le vocabulaire), et convertir chaque mot en un nombre, en recherchant son index dans le vocabulaire\n",
    "- Création d'un DataLoader pour le language model : fastai fournit une classe LMDataLoader qui gère automatiquement la création d'une variable à prédire qui est décalée de la variable d'entrée d'un token. Elle gère également certains détails importants, tels que la manière de mélanger les données d'apprentissage de manière à ce que les variables dépendantes et indépendantes conservent leur structure\n",
    "- Création d'un laguage model : Nous avons besoin d'un type de modèle spécial qui fait quelque chose que nous n'avons jamais vu auparavant : il gère des listes de saisie qui peuvent être arbitrairement grandes ou petites. Il existe plusieurs façons de le faire ; dans ce chapitre, nous utiliserons un réseau neuronal récurrent (RNN)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lorsque nous avons parlé de \"convertir le texte en une liste de mots\", nous avons omis beaucoup de détails. Par exemple, que faisons-nous de la ponctuation ? Comment traiter un mot comme \"don't\" ? S'agit-il d'un seul mot ou de deux ? Qu'en est-il des longs mots médicaux ou chimiques ? Doivent-ils être séparés en parties distinctes ? Et les mots avec un trait d'union ? Qu'en est-il des langues comme l'allemand et le polonais, où nous pouvons créer des mots vraiment longs à partir de nombreux morceaux ? Qu'en est-il des langues comme le japonais et le chinois qui n'utilisent pas du tout d'espaces et qui n'ont pas vraiment une idée bien définie du mot ?\n",
    "\n",
    "Comme il n'y a pas de réponse unique à ces questions, il n'y a pas d'approche unique de la tokenisaton. Il existe trois approches principales :\n",
    "\n",
    "- Word-based : Découper le texte selon les espaces, et appliquer des règles spécifiques à la langue pour essayer de séparer des parties qui ont un sens propre même lorsqu'il n'y a pas d'espaces (comme transformer \"don't\" en \"do not\"). En général, les signes de ponctuation sont également divisés en tokens séparés.\n",
    "- Subword-based : Découper les mots en plus petits morceaux, en vous basant sur les sous-chaînes les plus courantes. Par exemple, \"préentrainement\" peut être découpé en \"pré|entrain|ement\".\n",
    "- Character-based : Découper une phrase en ses différents caractères."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Tokenization with fastai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plutôt que de fournir son propre tokenizer, fastai fournit une interface cohérente avec des tokenizers fournis par des bibliothèques externes spécialisées. La tokenisation est un domaine de recherche actif, et des tokenizers nouveaux et améliorés sortent en permanence, de sorte que les paramètres par défaut utilisés par fastai changent également. Cependant, l'API et les options ne devraient pas trop changer, car fastai essaie de maintenir une API cohérente même si la technologie sous-jacente change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display,HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai2.text.all import *\n",
    "path = untar_data(URLs.IMDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = get_text_files(path, folders = ['train', 'test', 'unsup'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = files[0].open().read(); txt[:75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy = WordTokenizer()\n",
    "toks = first(spacy([txt]))\n",
    "print(coll_repr(toks, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first(spacy(['The U.S. dollar $1 is $1.00.']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tkn = Tokenizer(spacy)\n",
    "print(coll_repr(tkn(txt), 31))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "defaults.text_proc_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coll_repr(tkn('&copy;   Fast.ai www.fast.ai/INDEX'), 31)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subword Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txts = L(o.open().read() for o in files[:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subword(sz):\n",
    "    sp = SubwordTokenizer(vocab_sz=sz)\n",
    "    sp.setup(txts)\n",
    "    return ' '.join(first(sp([txt]))[:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subword(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subword(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subword(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numericalization with fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toks = tkn(txt)\n",
    "print(coll_repr(tkn(txt), 31))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toks200 = txts[:200].map(tkn)\n",
    "toks200[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = Numericalize()\n",
    "num.setup(toks200)\n",
    "coll_repr(num.vocab,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums = num(toks)[:20]; nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join(num.vocab[o] for o in nums)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting Our Texts into Batches for a Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "stream = \"In this chapter, we will go back over the example of classifying movie reviews we studied in chapter 1 and dig deeper under the surface. First we will look at the processing steps necessary to convert text into numbers and how to customize it. By doing this, we'll have another example of the PreProcessor used in the data block API.\\nThen we will study how we build a language model and train it for a while.\"\n",
    "tokens = tkn(stream)\n",
    "bs,seq_len = 6,15\n",
    "d_tokens = np.array([tokens[i*seq_len:(i+1)*seq_len] for i in range(bs)])\n",
    "df = pd.DataFrame(d_tokens)\n",
    "display(HTML(df.to_html(index=False,header=None)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "bs,seq_len = 6,5\n",
    "d_tokens = np.array([tokens[i*15:i*15+seq_len] for i in range(bs)])\n",
    "df = pd.DataFrame(d_tokens)\n",
    "display(HTML(df.to_html(index=False,header=None)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "bs,seq_len = 6,5\n",
    "d_tokens = np.array([tokens[i*15+seq_len:i*15+2*seq_len] for i in range(bs)])\n",
    "df = pd.DataFrame(d_tokens)\n",
    "display(HTML(df.to_html(index=False,header=None)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "bs,seq_len = 6,5\n",
    "d_tokens = np.array([tokens[i*15+10:i*15+15] for i in range(bs)])\n",
    "df = pd.DataFrame(d_tokens)\n",
    "display(HTML(df.to_html(index=False,header=None)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums200 = toks200.map(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = LMDataLoader(nums200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = first(dl)\n",
    "x.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join(num.vocab[o] for o in x[0][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join(num.vocab[o] for o in y[0][:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Text Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Model Using DataBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_imdb = partial(get_text_files, folders=['train', 'test', 'unsup'])\n",
    "\n",
    "dls_lm = DataBlock(\n",
    "    blocks=TextBlock.from_folder(path, is_lm=True),\n",
    "    get_items=get_imdb, splitter=RandomSplitter(0.1)\n",
    ").dataloaders(path, path=path, bs=128, seq_len=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls_lm.show_batch(max_n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuning the Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = language_model_learner(\n",
    "    dls_lm, AWD_LSTM, drop_mult=0.3, \n",
    "    metrics=[accuracy, Perplexity()]).to_fp16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(1, 2e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and Loading Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('1epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = learn.load('1epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(10, 2e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save_encoder('finetuned')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = \"I liked this movie because\"\n",
    "N_WORDS = 40\n",
    "N_SENTENCES = 2\n",
    "preds = [learn.predict(TEXT, N_WORDS, temperature=0.75) \n",
    "         for _ in range(N_SENTENCES)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\".join(preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Classifier DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls_clas = DataBlock(\n",
    "    blocks=(TextBlock.from_folder(path, vocab=dls_lm.vocab),CategoryBlock),\n",
    "    get_y = parent_label,\n",
    "    get_items=partial(get_text_files, folders=['train', 'test']),\n",
    "    splitter=GrandparentSplitter(valid_name='test')\n",
    ").dataloaders(path, path=path, bs=128, seq_len=72)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls_clas.show_batch(max_n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums_samp = toks200[:10].map(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums_samp.map(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = text_classifier_learner(dls_clas, AWD_LSTM, drop_mult=0.5, \n",
    "                                metrics=accuracy).to_fp16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = learn.load_encoder('finetuned')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuning the Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(1, 2e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.freeze_to(-2)\n",
    "learn.fit_one_cycle(1, slice(1e-2/(2.6**4),1e-2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.freeze_to(-3)\n",
    "learn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(2, slice(1e-3/(2.6**4),1e-3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disinformation and Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
